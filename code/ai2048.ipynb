{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMpJsf3D3DpMIjgg/iXSgE5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HEP-Dexan3327/AI-Diary-3-ReinforcementLearning/blob/main/code/ai2048.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KL_NU-dTxmQ0"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "import gym\n",
        "from gym import spaces\n",
        "import pygame\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\"Arrows Key to play, Ctrl + Z to undo. You can only undo once consecutively.\"\n",
        "\n",
        "\n",
        "class Typical2048Env(gym.Env):\n",
        "    metadata = {\"render_modes\": [\"ai\", \"human\", \"rgb_array\"], \"render_fps\": 20, \"window_size\": 16}\n",
        "\n",
        "    def __init__(self, render_mode=None, size=4, window_size=16):\n",
        "        self._grid = None\n",
        "        self._last_grid = None\n",
        "        self._merged = None\n",
        "        self._epoch = 0\n",
        "\n",
        "        self.size = size  # The size of the square grid\n",
        "        self.ws = window_size\n",
        "        self.window_size = 512 * window_size / 16  # The size of the PyGame window\n",
        "        self.bar_size = 100 * window_size / 16\n",
        "        self.bar = np.array((0, self.bar_size))\n",
        "\n",
        "        self.prob = (.9, .1)  # (.9, .1)\n",
        "        self.action = -1\n",
        "\n",
        "        self.reward_list_length = 1\n",
        "        self.score = 0\n",
        "        self.undo_score = 0\n",
        "        self.reward = 0\n",
        "        self.rewards = [0,]\n",
        "        self.max_score = 0\n",
        "        self.undo_unused = True\n",
        "        self.punishment = -50\n",
        "\n",
        "        self.available_dirs = np.array([True, True, True, True])\n",
        "\n",
        "        # Observations are 16-element lists, storing the numbers at each cell.\n",
        "        # There are 16 possible numbers, ranging from 2**1 to 2**16.\n",
        "        # The id 0 is reserved for EMPTY cell.\n",
        "        self.observation_space = spaces.Box(0, size * size, shape=(size * size,), dtype=int)\n",
        "\n",
        "        # We have 4 actions, corresponding to \"right\", \"down\", \"left\", \"up\" and \"undo\"\n",
        "        self.action_space = spaces.Discrete(4)\n",
        "\n",
        "        \"\"\"\n",
        "        The following dictionary maps abstract actions from `self.action_space` to \n",
        "        the direction we will walk in if that action is taken.\n",
        "        I.e. 0 corresponds to \"right\", 1 to \"down\" etc.\n",
        "        \"\"\"\n",
        "        self._action_to_direction = {\n",
        "            0: np.array([1, 1]),  # right  1st axis\n",
        "            1: np.array([0, 1]),  # down   0th axis\n",
        "            2: np.array([1, -1]),  # left   1st axis\n",
        "            3: np.array([0, -1]),  # up     0th axis\n",
        "        }\n",
        "\n",
        "        assert render_mode is None or render_mode in self.metadata[\"render_modes\"]\n",
        "        self.render_mode = render_mode\n",
        "        self.ai = render_mode == 'ai'\n",
        "        if self.ai:\n",
        "            self.render_mode = 'human'\n",
        "\n",
        "        \"\"\"\n",
        "        If human-rendering is used, `self.window` will be a reference\n",
        "        to the window that we draw to. `self.clock` will be a clock that is used\n",
        "        to ensure that the environment is rendered at the correct framerate in\n",
        "        human-mode. They will remain `None` until human-mode is used for the\n",
        "        first time.\n",
        "        \"\"\"\n",
        "        self.window = None\n",
        "        self.clock = None\n",
        "\n",
        "    def _get_obs(self):\n",
        "        return self._grid\n",
        "\n",
        "    def _get_info(self):\n",
        "        return {\n",
        "            \"highTile\": max(self._grid),\n",
        "            \"score\": self.score,\n",
        "            \"available_dir\": self.available_dirs\n",
        "        }\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        # We need the following line to seed self.np_random\n",
        "        super().reset(seed=seed)\n",
        "        # self.np_random.integers\n",
        "        if self._epoch != 0:\n",
        "            if self.render_mode == 'human':\n",
        "                # The following line copies our drawings from `canvas` to the visible window\n",
        "                s = pygame.Surface((self.window_size, self.window_size+self.bar_size))\n",
        "                s.set_alpha(196)\n",
        "                s.fill((64, 64, 64))\n",
        "                pygame.display.update(self.window.blit(s, (0,0)))\n",
        "                self._print_text(self.window, f'Game Over!', (self.window_size/2,self.window_size/2),\n",
        "                                 color=(255, 255, 255))\n",
        "                self._print_text(self.window, f'Your Score is {self.score}.',\n",
        "                                 (self.window_size/2,self.window_size/2+100), color=(255, 255, 255))\n",
        "                self.window.blit(self.window, self.window.get_rect())\n",
        "                pygame.event.pump()\n",
        "                pygame.display.update()\n",
        "                if not self.ai:\n",
        "                    time.sleep(5)\n",
        "\n",
        "                # We need to ensure that human-rendering occurs at the predefined framerate.\n",
        "                # The following line will automatically add a delay to keep the framerate stable.\n",
        "                self.clock.tick(self.metadata[\"render_fps\"])\n",
        "        self._epoch += 1\n",
        "\n",
        "        self.max_score = max(self.max_score, self.score)\n",
        "\n",
        "        self.punishment = -50\n",
        "        self.action = -1\n",
        "        self.score = 0\n",
        "        self.reward = 0\n",
        "        self.undo_score = 0\n",
        "        self.rewards = [0,]\n",
        "        self.available_dirs = np.array([True, True, True, True])\n",
        "        self.undo_unused = True\n",
        "        # Spawn the grid with 2 random tiles (2 or 4, i.e. code = 1 or 2)\n",
        "        self._grid = np.random.permutation((0,) * (self.size ** 2 - 2) +\n",
        "                                           tuple(np.random.choice((1, 2), size=(2,), p=self.prob)))\n",
        "        self._last_grid = self._grid.copy()\n",
        "        self._merged = np.zeros((self.size**2,))\n",
        "\n",
        "        observation = self._get_obs()\n",
        "        info = self._get_info()\n",
        "\n",
        "        if self.render_mode == \"human\":\n",
        "            self._render_frame()\n",
        "\n",
        "        return observation, info\n",
        "\n",
        "    def _move_row(self, p: np.ndarray, m: np.ndarray, direction: int, do_reward=False) -> tuple[bool, np.ndarray]:\n",
        "        \"\"\"\n",
        "        :param p: an ndarray row representing the numbers in a row (or column)\n",
        "        :param m: an ndarray row representing whether the number is JUST merged.\n",
        "        :param direction: an integer signifying whether to slide in the POSITIVE (+1) or NEGATIVE (-1) direction.\n",
        "        :param do_reward: an bool signifying if we update the reward value.\n",
        "        \"\"\"\n",
        "        out = np.zeros_like(p)\n",
        "        mout = np.zeros_like(m)\n",
        "        last = 0\n",
        "        lastidx = 0\n",
        "        direction *= -1\n",
        "        # if direction is -1, i.e., towards the LEFT, then we DON'T need to reverse the array.\n",
        "        # similarly, we NEED to reverse the row if direction is 1.\n",
        "\n",
        "        idx = -1\n",
        "        for i, e in enumerate(p[::direction]):\n",
        "            if e == 0:\n",
        "                continue\n",
        "            if e != last or m[i] != 0 or m[lastidx] != 0:  # either not equal, or one of them is used.\n",
        "                idx += 1\n",
        "                out[idx] = e\n",
        "                mout[idx] = m[i]\n",
        "                last = int(e)\n",
        "            else:\n",
        "                out[idx] = last+1   # merge tiles\n",
        "                mout[idx] = 1\n",
        "                if do_reward:\n",
        "                    self.reward += 2 ** (last + 1)\n",
        "                    self.score += 2 ** (last + 1)\n",
        "                last = 0\n",
        "        m[:] = mout\n",
        "        return not np.all(p-out[::direction] == 0), out[::direction]\n",
        "\n",
        "    def _move_tiles(self, grid: np.ndarray, merge_grid: np.ndarray, action=0, do_reward=False) -> bool:\n",
        "        g = grid.reshape((self.size, self.size))\n",
        "        mg = merge_grid.reshape((self.size, self.size))\n",
        "        # Map the action (element of {0,1,2,3}) to the direction we walk in\n",
        "        direction = self._action_to_direction[action]\n",
        "        # direction[0]: axis.\n",
        "        # direction[1]: +/- value for that axis.\n",
        "        OUT = False\n",
        "        for i in range(self.size):\n",
        "            p = g[:, i] if direction[0] else g[i, :]\n",
        "            m = mg[:, i] if direction[0] else mg[i, :]\n",
        "            out, p[:] = self._move_row(p, m, direction[1], do_reward=do_reward)\n",
        "            if out:\n",
        "                OUT = True\n",
        "                if not do_reward:\n",
        "                    return True\n",
        "\n",
        "        grid[:] = g.reshape((-1,))\n",
        "        return OUT\n",
        "\n",
        "    def is_full(self) -> bool:\n",
        "        ar = [self._move_tiles(self._grid.copy(), self._merged.copy(), action=action) for action in range(4)]\n",
        "        self.available_dirs = np.array(ar)# + [self.undo_unused])\n",
        "        return not (np.any(self.available_dirs))\n",
        "\n",
        "    def _spawn_tile(self):\n",
        "        empty_tiles = self._grid[self._grid == 0]\n",
        "        self._grid[self._grid == 0] = np.random.permutation((0,)*(len(empty_tiles)-1) +\n",
        "                                                            tuple(np.random.choice((1, 2), size=(1,), p=self.prob)))\n",
        "\n",
        "    def step(self, action):\n",
        "        self.reward = 0\n",
        "        self.action = action\n",
        "        if action == 4:\n",
        "            if self.undo_unused:\n",
        "                self._grid[:] = self._last_grid.copy()\n",
        "                self.score = self.undo_score\n",
        "                self.undo_unused = False\n",
        "                self.reward = self.punishment / 10\n",
        "            else:\n",
        "                self.reward = self.punishment\n",
        "                self.punishment -= 50\n",
        "        elif action is not None:\n",
        "            self._last_grid = self._grid.copy()\n",
        "            self.undo_score = self.score\n",
        "            self.undo_unused = True\n",
        "            if not self.is_full():\n",
        "                moved = 0\n",
        "                while self._move_tiles(self._grid, self._merged, action=action, do_reward=True):\n",
        "                    moved = 1\n",
        "                self._merged = np.zeros_like(self._merged)\n",
        "                if moved:\n",
        "                    self._spawn_tile()\n",
        "                else:\n",
        "                    self.reward = 0\n",
        "        # An episode is done iff the agent has reached the target\n",
        "        terminated = self.is_full()\n",
        "        if terminated:\n",
        "            self.reward -= 10\n",
        "        self.rewards += [self.reward]\n",
        "        if len(self.rewards) > self.reward_list_length:\n",
        "            self.rewards = self.rewards[1:]\n",
        "        observation = self._get_obs()\n",
        "        info = self._get_info()\n",
        "\n",
        "        if self.render_mode == \"human\":\n",
        "            self._render_frame()\n",
        "\n",
        "        return observation, self.reward, terminated, False, info\n",
        "\n",
        "    def _get_color(self, tile: int) -> tuple[int, int, int]:\n",
        "        colors = (\n",
        "            (237, 228, 218),  # 2,      1\n",
        "            (236, 223, 199),  # 4,      2\n",
        "            (243, 177, 121),  # 8,      3\n",
        "            (245, 149, 99),   # 16,     4\n",
        "            (245, 124, 97),   # 32,     5\n",
        "            (237, 87, 55),    # 64,     6\n",
        "            (236, 206, 113),  # 128,    7\n",
        "            (237, 204, 98),   # 256,    8\n",
        "            (236, 199, 80),   # 512,    9\n",
        "            (236, 197, 64),   # 1024,   10\n",
        "            (236, 197, 1),    # 2048,   11\n",
        "            (94, 220, 151),   # 4096,   12\n",
        "            (236, 77, 88),    # 8192,   13\n",
        "            (37, 186, 99),    # 16384,  14\n",
        "            (0, 124, 189),    # 32768,  15\n",
        "            (0, 0, 0)         # 65536,  16\n",
        "        )\n",
        "        return colors[tile-1]\n",
        "\n",
        "    def render(self):\n",
        "        if self.render_mode == \"rgb_array\":\n",
        "            return self._render_frame()\n",
        "\n",
        "    def _debug_text(self, canvas: pygame.Surface, text: str, pos=(300, 300), color=(150, 100, 100)):\n",
        "        self.__print_some_text(self.debug_font, canvas, text, pos, color)\n",
        "\n",
        "    def _print_text(self, canvas: pygame.Surface, text: str, pos=(300, 300), color=(100, 100, 100)):\n",
        "        self.__print_some_text(self.font, canvas, text, pos, color)\n",
        "\n",
        "    def __print_some_text(self, font, canvas: pygame.Surface, text: str, pos=(300, 300), color=(100, 100, 100)):\n",
        "        text_surf = font.render(text, False, color)\n",
        "        text_rect = text_surf.get_rect()\n",
        "        text_rect.center = pos\n",
        "        canvas.blit(text_surf, text_rect)\n",
        "\n",
        "    def _render_frame(self):\n",
        "        if self.window is None and self.render_mode in (\"human\", \"rgb_array\"):\n",
        "            pygame.init()\n",
        "            pygame.display.init()\n",
        "            self.window = pygame.display.set_mode((self.window_size, self.window_size+self.bar_size))\n",
        "            self.font = pygame.font.SysFont('Garamond', 50 * self.ws // 16)\n",
        "            self.debug_font = pygame.font.SysFont('Garamond', 20 * self.ws // 16)\n",
        "        if self.clock is None and self.render_mode == \"human\":\n",
        "            self.clock = pygame.time.Clock()\n",
        "\n",
        "        canvas = pygame.Surface((self.window_size, self.window_size+self.bar_size))\n",
        "        canvas.fill((255, 255, 255))\n",
        "        pix_square_size = (\n",
        "                self.window_size / self.size\n",
        "        )  # The size of a single grid square in pixels\n",
        "        margin = pix_square_size * .1\n",
        "\n",
        "        # Drawing the tiles.\n",
        "        for i in range(self.size):\n",
        "            for j in range(self.size):\n",
        "                tile = self._grid[i*self.size + j]\n",
        "                if tile != 0:\n",
        "                    rect = pygame.Rect(\n",
        "                            self.bar + pix_square_size * np.array((i, j)) + margin,\n",
        "                            (pix_square_size - margin * 2, pix_square_size - margin * 2),\n",
        "                        )\n",
        "                    pygame.draw.rect(\n",
        "                        canvas,\n",
        "                        self._get_color(tile),\n",
        "                        rect,\n",
        "                    )\n",
        "                    self._print_text(canvas, str(2 ** tile), pos=rect.center)\n",
        "\n",
        "        # Finally, add some gridlines\n",
        "        for x in range(self.size + 1):\n",
        "            pygame.draw.line(\n",
        "                canvas,\n",
        "                0,\n",
        "                (0, self.bar_size+pix_square_size * x),\n",
        "                (self.window_size, self.bar_size+pix_square_size * x),\n",
        "                width=3,\n",
        "            )\n",
        "            pygame.draw.line(\n",
        "                canvas,\n",
        "                0,\n",
        "                (pix_square_size * x, self.bar_size),\n",
        "                (pix_square_size * x, self.window_size+self.bar_size),\n",
        "                width=3,\n",
        "            )\n",
        "\n",
        "        self._print_text(canvas, f'Score: {self.score}   Hi: {self.max_score}',\n",
        "                         pos=(self.window_size/2, self.bar_size/2))\n",
        "        self._debug_text(canvas, f'Action: {self.action}', pos=(self.window_size/2, self.bar_size/4))\n",
        "        self._debug_text(canvas, f'Available: {self.available_dirs}', pos=(self.window_size / 2, self.bar_size * 3 / 4))\n",
        "\n",
        "        if self.render_mode == \"human\":\n",
        "            # The following line copies our drawings from `canvas` to the visible window\n",
        "            self.window.blit(canvas, canvas.get_rect())\n",
        "            pygame.event.pump()\n",
        "            pygame.display.update()\n",
        "\n",
        "            # We need to ensure that human-rendering occurs at the predefined framerate.\n",
        "            # The following line will automatically add a delay to keep the framerate stable.\n",
        "            self.clock.tick(self.metadata[\"render_fps\"])\n",
        "        else:  # rgb_array\n",
        "            return np.transpose(\n",
        "                np.array(pygame.surfarray.pixels3d(canvas)), axes=(1, 0, 2)\n",
        "            )\n",
        "\n",
        "    def close(self):\n",
        "        if self.window is not None:\n",
        "            pygame.display.quit()\n",
        "            pygame.quit()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import os\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from itertools import filterfalse\n",
        "\n",
        "\n",
        "class Output:\n",
        "    has_epsilon = False\n",
        "\n",
        "    def __init__(self, file_input: str, mode: str = 'results', output_every_n=100, random_overlay=False):\n",
        "        \"\"\" mode: either 'results' or 'model'.\n",
        "            ... -o '<path>/<file>.txt'  # outputs experiment data to txt format\n",
        "            ... -o '<path>/<file>.csv'  # outputs experiment data to csv format\n",
        "            ... -o '<path>/<file>.png'  # outputs experiment data chart (provided matplotlib) to png or jpg format\n",
        "            ... -o '<path>/<file>.h5'   # when mode == 'model', outputs neural network model here.\n",
        "            ... -o '<path>'             # when there is no file extension (.xx),\n",
        "                                        # a .txt file, a .csv file and (if mode == 'model')\n",
        "                                            a list of .h5 file checkpoints are stored in this folder.\n",
        "            ... -o None\n",
        "             or -o 'None'               # show plotted chart on screen\n",
        "\n",
        "            # by default, there will be printing onto the console. there is no way to turn it off.\n",
        "            \"\"\"\n",
        "        self.mode = mode\n",
        "        Output.output = (file_input is not None)\n",
        "        if not self.output:\n",
        "            return\n",
        "\n",
        "        file = file_input.rsplit(\".\", 1)\n",
        "        file[0] = \"./\" + file[0]\n",
        "        self.files = {}\n",
        "        self.output_every_n = output_every_n\n",
        "        self.random_overlay = random_overlay\n",
        "        self.df = pd.DataFrame({'episode': [],\n",
        "                                'step': [],\n",
        "                                'score': [], 'epsilon': [], 'good': [], 'best': []})\n",
        "        if file_input.endswith(\".txt\"):\n",
        "            self.files.update({\"txt\": file[0].rsplit('/', 1)[1]})\n",
        "        elif file_input.endswith(\".csv\"):\n",
        "            self.files.update({\"csv\": file[0].rsplit('/', 1)[1]})\n",
        "        elif file_input.endswith(\".png\"):\n",
        "            self.files.update({\"png\": file[0].rsplit('/', 1)[1]})\n",
        "        elif file_input.endswith(\".jpg\"):\n",
        "            self.files.update({\"jpg\": file[0].rsplit('/', 1)[1]})\n",
        "        elif file_input.endswith(\".h5\") and mode == 'model':\n",
        "            self.save_to_single = True\n",
        "            self.files.update({\"h5\": file[0].rsplit('/', 1)[1]})\n",
        "        elif file_input.endswith(\".h5s\") and mode == 'model':\n",
        "            self.save_to_single = False\n",
        "            self.files.update({\"h5\": file[0].rsplit('/', 1)[1]})\n",
        "        else:\n",
        "            file[0] = file[0][2:] if file[0].startswith(\"./\") else file[0]\n",
        "            if \".\" not in file_input: # is a directory\n",
        "                self.files.update({\"txt\": \"data\", \"csv\": \"data\", \"png\": \"evaluation_graph\"})\n",
        "                if mode == 'model':\n",
        "                    self.files.update({\"h5\": \"model\"})\n",
        "                file = [file_input+\"/\"]\n",
        "            else:\n",
        "                print(f\"File format {file[1]} not supported. \"\n",
        "                      f\"Train data and files (if any) is now saved to the directory {file[0]}.\")\n",
        "            try:\n",
        "                os.mkdir(file[0])\n",
        "            except Exception as e:\n",
        "                print(e)\n",
        "        file[0] = file[0][2:] if file[0].startswith(\"./\") else file[0]\n",
        "        file_input = file[0].rsplit(\"/\", 1) if \"/\" in file[0] else (\".\", file[0])\n",
        "        # ignore file format if the format is not supported\n",
        "        self.dir = file_input[0]\n",
        "        print(f\"The following items will be outputted in folder {self.dir}:\\n\"\n",
        "              + '\\n'.join(map(lambda x: '\\t- '+x[1]+'.'+x[0], self.files.items())))\n",
        "\n",
        "        self.actions = {\"txt\": self.output_txt, \"csv\": self.output_csv, \"png\": self.output_img,\n",
        "                        \"jpg\": self.output_img, \"h5\": self.output_model}\n",
        "        self.actions = dict(list(filterfalse(lambda x: x[0] not in self.files, self.actions.items())))\n",
        "\n",
        "        matplotlib.use('Agg')\n",
        "\n",
        "    @staticmethod\n",
        "    def check(func):\n",
        "        def inner(*args, **kwargs):\n",
        "            if not Output.output:\n",
        "                return\n",
        "            return func(*args, **kwargs)\n",
        "        return inner\n",
        "\n",
        "    @check\n",
        "    def output_txt(self, output=\"\", **kwargs):\n",
        "        with open(f\"{self.dir}/{self.files['txt']}.txt\", \"a+\") as f:\n",
        "            f.write(output + \"\\n\")\n",
        "\n",
        "    @check\n",
        "    def output_csv(self, episode=0, step=0, info=None, **kwargs):\n",
        "        self.df.to_csv(f\"{self.dir}/{self.files['csv']}.csv\")\n",
        "\n",
        "    @check\n",
        "    def output_img(self, episode=0, output_every_n=1, do_output=True, **kwargs):\n",
        "        if self.mode != 'model' and episode >= 0 or not do_output:\n",
        "            return\n",
        "        \"\"\"\n",
        "        # there is some issue with plt and pygame.\n",
        "        # plt resizes the pygame window by ignoring Window's screen resize ratio (200% in my display)\n",
        "        # and thus shrinks the window size of pygame after every plt call.\n",
        "        # Currently, I cannot find any resources online that could help me solve this problem, so\n",
        "        # this would NOT be implemented directly. Only the last data.png is saved.\n",
        "        \"\"\"\n",
        "        if episode % output_every_n != output_every_n-1 and episode >= 0:\n",
        "            return\n",
        "        mode = 'png' if 'png' in self.files else 'jpg'\n",
        "\n",
        "        fig, axs = plt.subplots(nrows=3 if self.has_epsilon else 2, ncols=2,\n",
        "                                figsize=(8, 9 if self.has_epsilon else 6), num=1, clear=True)\n",
        "        fig.tight_layout()\n",
        "        self.df = self.df.sort_values(by='episode')\n",
        "        self.df.set_index('episode')\n",
        "        rolling = self.df.rolling(10, on='episode').mean()\n",
        "\n",
        "        # 'best' is used as 'random_score' when self.random_overlay is True.\n",
        "        axs[0, 0].plot(rolling['score'].dropna())\n",
        "        try:\n",
        "            if self.random_overlay:\n",
        "                axs[0, 0].plot(rolling['best'], alpha=.5)\n",
        "                axs[0, 0].legend(['score', 'random'])\n",
        "            else:\n",
        "                axs[0, 0].plot(self.df['good'].dropna())\n",
        "                axs[0, 0].plot(self.df['best'].dropna())\n",
        "                axs[0, 0].legend(['score', 'good', 'best'])\n",
        "        except KeyError:\n",
        "            pass\n",
        "        magicmax = max(5000, (self.df['score'].max()//1000+1) * 1000)\n",
        "        self.df['score'].hist(ax=axs[0, 1], range=[0, magicmax], bins=100)\n",
        "        if self.random_overlay:\n",
        "            self.df['best'].hist(ax=axs[0, 1], range=[0, magicmax], bins=100, alpha=0.5)\n",
        "        axs[0, 1].set_xlabel(\"score\")\n",
        "        axs[0, 1].set_ylabel(\"frequency\")\n",
        "\n",
        "        # 'good' is used as 'random_step' when self.random_overlay is True.\n",
        "        axs[1, 0].plot(rolling['step'].dropna())\n",
        "        if self.random_overlay:\n",
        "            axs[1, 0].plot(rolling['good'], alpha=.5)\n",
        "            axs[1, 0].legend(['step', 'random'])\n",
        "        self.df['step'].hist(ax=axs[1, 1], range=[-5, 2000], bins=100)\n",
        "        if self.random_overlay:\n",
        "            self.df['good'].hist(ax=axs[1, 1], range=[-5, 2000], bins=100, alpha=0.5)\n",
        "        axs[1, 1].set_xlabel(\"step\")\n",
        "        axs[1, 1].set_ylabel(\"frequency\")\n",
        "        if self.has_epsilon:\n",
        "            axs[2, 0].set_ylim([0, 1])\n",
        "            for i in range(15, -1, -1):\n",
        "                self.df[f'epsilon{i}'].plot(ax=axs[2, 0], label=f\"{2**i}\")\n",
        "                self.df[f'epsilon{i}'].hist(ax=axs[2, 1], range=[0, 1], bins=100, label=f\"{2**i}\", alpha=.5)\n",
        "            axs[2, 1].set_xlabel(\"epsilon\")\n",
        "            axs[2, 1].set_ylabel(\"frequency\")\n",
        "            axs[2, 1].legend()\n",
        "        del rolling\n",
        "\n",
        "        plt.savefig(f\"{self.dir}/{self.files[mode]}.{mode}\")\n",
        "        plt.close(fig)\n",
        "        plt.close(\"all\")\n",
        "        gc.collect()\n",
        "\n",
        "    @check\n",
        "    def output_model(self, episode=0, model=None, output_every_n=1, **kwargs):\n",
        "        if episode % output_every_n != output_every_n-1 and episode >= 0:\n",
        "            return\n",
        "        if episode == -1:\n",
        "            filename = f\"{self.dir}/{self.files['h5']}_best.h5\"\n",
        "        elif episode < 0:\n",
        "            filename = f\"{self.dir}/{self.files['h5']}_best_replacedAt{-episode}.h5\"\n",
        "        else:\n",
        "            filename = f\"{self.dir}/{self.files['h5']}_epoch{episode}.h5\"\n",
        "        model.save(filename)\n",
        "\n",
        "    def log(self, done, episode, step, info, model=None, do_output=True, epsilon: list[float] = [-1.], training=False,\n",
        "            best: int = None):\n",
        "        if done:\n",
        "            output = f\"Episode {episode} succeeded in {step} steps with score {info['score']}... epsilon {epsilon} ... training {training}\"\n",
        "        else:\n",
        "            output = f\"Episode {episode} truncated ... in {step} steps with score {info['score']} ... epsilon {epsilon}\"\n",
        "\n",
        "        print(output)\n",
        "\n",
        "        if not self.output:\n",
        "            return\n",
        "        self.has_epsilon = epsilon[0] != -1\n",
        "        entry = {'episode': [episode], 'step': [step],\n",
        "                 'score': [info['score']], 'best': [np.NAN], 'good': [np.NAN]}\n",
        "        entry.update({f'epsilon{i}': eps for i, eps in enumerate(epsilon)})\n",
        "        self.concat(entry)\n",
        "        [a(output=output, episode=episode, step=step, info=info, output_every_n=self.output_every_n,\n",
        "           model=model, do_output=do_output) for a in self.actions.values()]\n",
        "\n",
        "    @check\n",
        "    def logs(self, output: str):\n",
        "        if \"txt\" in self.actions:\n",
        "            self.output_txt(output)\n",
        "\n",
        "    def concat(self, dic: dict):\n",
        "        self.df = pd.concat(\n",
        "            [self.df, pd.DataFrame(dic)],\n",
        "            ignore_index=True\n",
        "        ).groupby('episode').sum(numeric_only=True, min_count=1).reset_index()"
      ],
      "metadata": {
        "id": "2dc6cCDU1jXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "Experience = collections.namedtuple('Experience',\n",
        "                                    field_names=['state', 'action', 'reward', 'done', 'new_state'])\n",
        "\n",
        "\n",
        "class ExperienceReplay:\n",
        "    \"\"\"\n",
        "    Reference: https://towardsdatascience.com/deep-q-network-dqn-ii-b6bf911b6b2c\n",
        "    There is not much that I could improve the code. @credit: Jordi TORRES.AI\n",
        "    \"\"\"\n",
        "    def __init__(self, capacity, best_capacity=256):\n",
        "        self.buffer = collections.deque(maxlen=capacity)\n",
        "        self.best = []\n",
        "        self.age = 0\n",
        "        self.best_capacity = best_capacity\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def append(self, experience):\n",
        "        self.age += 1\n",
        "        self.buffer.append(experience)\n",
        "        # based on the concept that high reward should be prioritized\n",
        "        # we would make sure that older experiences with high reward values is not dumped\n",
        "        self.best += [(self.age, experience)]\n",
        "        self.best = sorted(self.best, key=lambda x: x[1][2]+x[0]/10000, reverse=True)\n",
        "        if len(self.best) > self.best_capacity:\n",
        "            self.best.pop()\n",
        "        # i.e. reward + age/1000\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        indices = np.random.choice(len(self.buffer), batch_size,\n",
        "                                   replace=False)\n",
        "        states, actions, rewards, dones, next_states \\\n",
        "            = zip(*([self.buffer[idx] for idx in indices]\n",
        "                    + [x[1] for x in self.best]))\n",
        "\n",
        "        return np.array(states), np.array(actions), \\\n",
        "               np.array(rewards, dtype=np.float32), \\\n",
        "               np.array(dones, dtype=np.uint8), \\\n",
        "               np.array(next_states)"
      ],
      "metadata": {
        "id": "a0NJEBUo1oaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def choose(env: gym.Env, _q_values: tf.Tensor, available_dirs: np.ndarray) -> np.ndarray:\n",
        "    if env.is_full():\n",
        "        return np.array(-1)\n",
        "    li = (_q_values[0] * available_dirs).numpy()\n",
        "    li[available_dirs*1 == 0] = np.nan\n",
        "    return np.nanargmax(li)\n",
        "\n",
        "\n",
        "num_inps = 4\n",
        "\n",
        "\n",
        "def vectorize(_state: tf.Tensor, available_dirs: np.ndarray, type='normal', normalized=True, expand=True) -> tf.Tensor:\n",
        "    \"\"\" turns the state into a vector before feeding into the neural network.\n",
        "    :param available_dirs: available directions.\n",
        "    :param _state:         input observations, in a shape (16,)\n",
        "    :param type:           either 'normal' (default) or 'one-hot'.\n",
        "                            output shape is (1,16,1) for 'normal', and is (1,16,16=#options) for 'one-hot'.\n",
        "    \"\"\"\n",
        "    if type in ('one-hot', 'one-hot-17'):\n",
        "        options_per_cell = 17 if type == 'one-hot-17' else 16\n",
        "        out = tf.math.multiply(tf.expand_dims(tf.one_hot(_state, options_per_cell), 0),\n",
        "                               available_dirs.reshape((num_inps, 1, 1)))\n",
        "        if expand:\n",
        "            out = tf.expand_dims(out, 0)\n",
        "        if normalized:\n",
        "            out /= options_per_cell\n",
        "        return out\n",
        "    out = tf.math.multiply(tf.expand_dims(_state, 0),\n",
        "                           available_dirs.reshape((num_inps, 1)))\n",
        "    if expand:\n",
        "        out = tf.expand_dims(out, 0)\n",
        "    return out\n",
        "\n",
        "\n",
        "def get_input_type(shape: tuple) -> str:\n",
        "    return 'one-hot' if shape[2] == 16 else \\\n",
        "        'one-hot-17' if shape[2] == 17 else 'normal'"
      ],
      "metadata": {
        "id": "0FIHaE2F1xEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from collections import Counter\n",
        "\n",
        "import gym\n",
        "import pygame\n",
        "\n",
        "import sys\n",
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Reshape, Conv2D\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "import copy\n",
        "import threading\n",
        "\n",
        "size = 4\n",
        "mode = 'rgb_array'\n",
        "ws   = 16\n",
        "\n",
        "save_interval = 500\n",
        "\n",
        "learning_rate = .00075\n",
        "lr_decay = 1\n",
        "opt = Adam(learning_rate=learning_rate)\n",
        "gamma = .95  # or .95\n",
        "epsilon_decay = 1-5*1e-4  # 1-5*1e-4\n",
        "local_epsilon = [.5]*16\n",
        "\n",
        "# for random agent, use play v1 script with -m human_rand\n",
        "assert mode != \"human_rand\"\n",
        "\n",
        "window_size = args.window_size\n",
        "\n",
        "env = Typical2048Env(render_mode=mode, size=size, window_size=window_size)\n",
        "\n",
        "env.metadata[\"render_fps\"] = 1000000000\n",
        "\n",
        "env.action_space.seed(None)\n",
        "\n",
        "\n",
        "#threading\n",
        "num_threads = 1 if args.mode=='human' else 10\n",
        "envs = [copy.deepcopy(env) for i in range(num_threads)]\n",
        "\n",
        "buffer_capacity = 32768\n",
        "best_capacity = 0\n",
        "min_buffer_length = 256\n",
        "buffer = ExperienceReplay(buffer_capacity, best_capacity=best_capacity)\n",
        "\n",
        "ofile = f\"data/qtable_{time.strftime('%Y%m%d%H%M')}\"\n",
        "output = Output(ofile, 'model', output_every_n=save_interval)  # does any output job.\n",
        "folder = output.dir\n",
        "\n",
        "# DQN model\n",
        "num_classes = 4\n",
        "options_per_cell = 16  # 16 if onehot / all models on or before 202212110239\n",
        "train_type = 'one-hot'  #'one-hot' if output 16\n",
        "input_shape = (num_classes, size ** 2, options_per_cell)\n",
        "epsilon_min = 0\n",
        "epsilon = .5\n",
        "\n",
        "file = None\n",
        "\n",
        "if file is not None:\n",
        "    model = tf.keras.models.load_model(file, compile=False)\n",
        "    print(model.summary())\n",
        "    train_type = get_input_type(model.shape)\n",
        "else:\n",
        "    \"Dimensionality reduction by obtaining Q-value rows by using a neural network.\"\n",
        "    model = Sequential(\n",
        "        [\n",
        "            tf.keras.Input(shape=input_shape),\n",
        "            Reshape((num_classes, size, size, options_per_cell)),\n",
        "            Conv2D(128, kernel_size=(2, 2), activation=\"relu\", padding='SAME'),\n",
        "            Conv2D(32, kernel_size=(2, 2), activation=\"relu\", padding='SAME'),\n",
        "            Reshape((-1,)),\n",
        "            Dense(128, activation='relu'),\n",
        "            Dense(num_classes)\n",
        "        ]\n",
        "    )\n",
        "    model.build()\n",
        "\n",
        "\n",
        "print(model.summary())\n",
        "with open(f\"{folder}/model_structure.txt\", \"a+\") as f:\n",
        "    model.summary(print_fn=lambda x: f.write(x + \"\\n\"))\n",
        "\n",
        "with open(f\"{folder}/model_structure.txt\", \"a+\") as f:\n",
        "    f.write(f\"Model trained from loaded file: {input_file}\\n\")\n",
        "    f.write(f\"Parameters: \\tbuffer size: {buffer_capacity}\\n\")\n",
        "    f.write(f\"\\t\\t\\t\\t* best capacity: {best_capacity}\\n\")\n",
        "    f.write(f\"\\t\\t\\t\\t* gamma: {gamma}, epsilon: {epsilon} (decay = {epsilon_decay}, min = {epsilon_min})\\n\")\n",
        "    f.write(f\"\\t\\t\\t\\t* lr: {learning_rate} (decay = {lr_decay})\\n\")\n",
        "    model.summary(print_fn=lambda x: f.write(x + \"\\n\"))\n",
        "\n",
        "\n",
        "def end():\n",
        "    if episode > 0:\n",
        "        print(f\"Average score: {total_score / episode:.2f}\\n\" +\n",
        "              f\"Maximum score: {max_score:d}\\n\" + f\"Highest tile: {2 ** high_tile:d}\\n\" +\n",
        "              f\"Average steps: {total_steps / episode:.2f} ([{min_steps_achieved} to {max_steps_achieved}])\")\n",
        "\n",
        "    env.close()\n",
        "\n",
        "\n",
        "# Test the agent\n",
        "test_episodes = args.episodes\n",
        "max_steps = args.max_steps\n",
        "\n",
        "episode = 0\n",
        "total_score = 0\n",
        "max_score = 0\n",
        "high_tile = 0\n",
        "total_steps = 0\n",
        "min_steps_achieved = (2 << 15)\n",
        "max_steps_achieved = 0\n",
        "\n",
        "len_top_tiles = 10  # maximum number of games that we are keeping track of (the high score)\n",
        "\n",
        "running = True\n",
        "step = 0\n",
        "\n",
        "top_tiles = []\n",
        "\n",
        "# threading\n",
        "class PlayModel (threading.Thread):\n",
        "    def __init__(self, env, episode):\n",
        "        threading.Thread.__init__(self)\n",
        "        self.env = env\n",
        "        self.episode = episode\n",
        "    def run(self):\n",
        "        global max_score, high_tile, total_score, top_tiles, epsilon, \\\n",
        "            max_steps_achieved, min_steps_achieved, total_steps, output\n",
        "        env = self.env\n",
        "        episode = self.episode\n",
        "\n",
        "        info = {'available_dir': np.array([True, True, True, True]), 'score': 0, 'highTile': 0}\n",
        "\n",
        "        state = env.reset(seed=args.seed)[0]  # [0] for observation only\n",
        "        state = vectorize(state, info['available_dir'], type=train_type)\n",
        "        total_testing_rewards = 0\n",
        "\n",
        "        for step in range(max_steps):\n",
        "            if args.mode != 'rgb_array':\n",
        "                events = pygame.event.get()\n",
        "                for event in events:\n",
        "                    if event.type == pygame.QUIT:\n",
        "                        output.output_model(episode, model)\n",
        "                        end()\n",
        "                        exit()\n",
        "\n",
        "            \"Obtain Q-values from network.\"\n",
        "            q_values = model(state)\n",
        "\n",
        "            \"Select action using epsilon-greedy strategy.\"\n",
        "            sample_epsilon = np.random.rand()\n",
        "            thisgame_hi = info['highTile']\n",
        "            self_epsilon = local_epsilon[thisgame_hi]\n",
        "            if sample_epsilon <= self_epsilon:\n",
        "                action = env.action_space.sample(mask=info['available_dir'].astype(np.int8))\n",
        "            else:\n",
        "                action = choose(env, q_values, info['available_dir'])\n",
        "            \"Obtain q-value for the selected action.\"\n",
        "            q_value = q_values[0, action]\n",
        "\n",
        "            \"Deterimine next state.\"\n",
        "            new_state, reward, done, truncated, info = env.step(action)  # take action and get reward\n",
        "            new_state = vectorize(new_state, info['available_dir'], type=train_type)\n",
        "            buffer.append(Experience(state, action, reward, done, new_state))\n",
        "\n",
        "            state = new_state\n",
        "\n",
        "            \"From the Q-learning update formula, we have:\"\n",
        "            \"   Q'(S, A) = Q(S, A) + a * {R + λ argmax[a, Q(S', a)] - Q(S, A)}\"\n",
        "            \"Target of Q' is given by: \"\n",
        "            \"   R + λ argmax[a, Q(S', a)]\"\n",
        "            \"Hence, MSE loss function is given by: \"\n",
        "            \"   L(w) = E[(R + λ argmax[a, Q(S', a, w)] - Q(S, a, w))**2]\"\n",
        "            next_q_values = model(new_state)\n",
        "            next_action = choose(env, next_q_values, info['available_dir'])\n",
        "            next_q_value = next_q_values[0, next_action]\n",
        "\n",
        "            observed_q_value = reward + (gamma * next_q_value)\n",
        "            loss = (observed_q_value - q_value) ** 2\n",
        "\n",
        "            def decay(ep: float) -> float:\n",
        "                ep *= epsilon_decay\n",
        "                return max(ep, epsilon_min)\n",
        "\n",
        "            self_epsilon = decay(self_epsilon)\n",
        "            epsilon = decay(epsilon)\n",
        "            for i in range(thisgame_hi+1):\n",
        "                local_epsilon[i] = min(decay(local_epsilon[i]), decay(local_epsilon[thisgame_hi]))\n",
        "\n",
        "            # print(state, action)\n",
        "            if done or truncated:\n",
        "                total_score += info['score']\n",
        "                max_score = max(max_score, info['score'])\n",
        "                high_tile = max(high_tile, info['highTile'])\n",
        "                top_tiles += [2 ** info['highTile']]\n",
        "                if len(top_tiles) > len_top_tiles:\n",
        "                    del top_tiles[0]\n",
        "\n",
        "                soutput = f\"Episode {episode} succeeded in {step} steps with score {info['score']},\" \\\n",
        "                          f\" high tile {2 ** info['highTile']}..., \\n\" \\\n",
        "                          f\"Highest tile frequencies: {top_tiles}\" \\\n",
        "                          f\"\\nepsilon: {self_epsilon}; q_values: {q_values}\"\n",
        "                print(soutput)\n",
        "\n",
        "                with open(f\"{folder}/_descriptions.txt\", \"a+\") as f:\n",
        "                    f.write(soutput + \"\\n\")\n",
        "                with open(f\"{folder}/_data.txt\", \"a+\") as f:\n",
        "                    f.write(f\"{episode}\\t{step}\\t{info['score']}\\t{info['highTile']}\\n\")\n",
        "\n",
        "                output.log(done, episode, step, info, model=model, do_output=False, epsilon=local_epsilon)\n",
        "\n",
        "                total_steps += step\n",
        "                max_steps_achieved = max(max_steps_achieved, step)\n",
        "                min_steps_achieved = min(min_steps_achieved, step)\n",
        "                break\n",
        "    def join(self):\n",
        "        threading.Thread.join(self)\n",
        "\n",
        "class TrainModel (threading.Thread):\n",
        "\n",
        "    def __init__(self, experience, episode):\n",
        "        threading.Thread.__init__(self)\n",
        "        self.experience = experience\n",
        "        self.episode = episode\n",
        "\n",
        "    @staticmethod\n",
        "    def squeeze(inp: np.ndarray):\n",
        "        return np.squeeze(inp, axis=1)\n",
        "\n",
        "    def run(self):\n",
        "        global max_score, total_score, epsilon, \\\n",
        "            max_steps_achieved, min_steps_achieved, total_steps, output, high\n",
        "        experience = self.experience\n",
        "        episode = self.episode\n",
        "        state, action, reward, done, new_state = experience\n",
        "\n",
        "        with tf.GradientTape() as tape:  # tracing and computing the gradients ourselves.\n",
        "            \"Obtain Q-values from network.\"\n",
        "            q_values = model(self.squeeze(state))\n",
        "\n",
        "            \"Obtain q-value for the selected action.\"\n",
        "            q_value = tf.gather(q_values, tf.constant(action), axis=1)#q_values[action]\n",
        "            #print(q_values)\n",
        "\n",
        "            \"From the Q-learning update formula, we have:\"\n",
        "            \"   Q'(S, A) = Q(S, A) + a * {R + λ argmax[a, Q(S', a)] - Q(S, A)}\"\n",
        "            \"Target of Q' is given by: \"\n",
        "            \"   R + λ argmax[a, Q(S', a)]\"\n",
        "            \"Hence, MSE loss function is given by: \"\n",
        "            \"   L(w) = E[(R + λ argmax[a, Q(S', a, w)] - Q(S, a, w))**2]\"\n",
        "            next_q_values = tf.stop_gradient(model(self.squeeze(new_state)))\n",
        "            next_actions = tf.math.argmax(next_q_values, 1)\n",
        "            next_q_value = tf.gather(next_q_values, next_actions, axis=1)\n",
        "\n",
        "            observed_q_value = reward + (gamma * next_q_value)\n",
        "            loss = (observed_q_value - q_value) ** 2\n",
        "\n",
        "            \"Computing and applying gradients\"\n",
        "            grads = tape.gradient(loss, model.trainable_variables)\n",
        "            opt.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "    def join(self):\n",
        "        threading.Thread.join(self)\n",
        "\n",
        "save_interval = 50\n",
        "train_episode = 0\n",
        "\n",
        "print(\"Training started ...\")\n",
        "trainThreads = []\n",
        "for episode in range(test_episodes):\n",
        "    thread = PlayModel(envs[episode % num_threads], episode)\n",
        "    thread.start()\n",
        "    trainThreads.append(thread)\n",
        "    if episode % num_threads == num_threads-1:\n",
        "        [trainThread.join() for trainThread in trainThreads]\n",
        "        trainThreads = []\n",
        "    if episode % save_interval > save_interval - 5:\n",
        "        output.concat({'episode': [episode], 'best': [np.NAN], 'good': [np.NAN]})\n",
        "        output.output_img(episode=-1)\n",
        "\n",
        "    if len(buffer) > min_buffer_length:\n",
        "        for j in range(num_threads):\n",
        "            thread = TrainModel(buffer.sample(512), train_episode)\n",
        "            thread.start()\n",
        "            trainThreads.append(thread)\n",
        "            if j == num_threads - 1:\n",
        "                [trainThread.join() for trainThread in trainThreads]\n",
        "                trainThreads = []\n",
        "            if train_episode % save_interval > save_interval - 5:\n",
        "                output.concat({'episode': [episode], 'best': [np.NAN], 'good': [np.NAN]})\n",
        "                output.output_img(episode=-1)\n",
        "            train_episode += 1\n",
        "\n",
        "\n",
        "end()\n"
      ],
      "metadata": {
        "id": "4Wf0x5YGxrEs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}